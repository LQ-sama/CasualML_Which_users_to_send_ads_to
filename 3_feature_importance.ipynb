{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from causalml.inference.meta import XGBTRegressor\n",
    "from causalml.inference.meta import BaseSLearner, BaseTLearner, BaseRLearner, BaseXLearner\n",
    "from causalml.inference.tree import UpliftRandomForestClassifier\n",
    "from causalml.dataset import *\n",
    "from causalml.metrics import *\n",
    "import sklearn\n",
    "from sklearn.ensemble import StackingRegressor,VotingRegressor,VotingClassifier,RandomForestRegressor\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from causalml.propensity import GradientBoostedPropensityModel \n",
    "from causalml.feature_selection.filters import FilterSelect\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据集，将特征和标签分开\n",
    "data = pd.read_csv('./data/-999_train.csv')  # 读取数据集\n",
    "data = data.iloc[:,1:]\n",
    "\n",
    "X = data.drop(['y','treatment'],axis=1) # 特征列\n",
    "y = data['y']  # 标签列\n",
    "treatment = data['treatment']  # 处理组标识\n",
    "feature_names = X.head(0)\n",
    "\n",
    "print('特征、标签、干预的大小为：',X.shape,y.shape,treatment.shape)\n",
    "# 将数据集分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(X, y, treatment, test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############使用随机森林特征重要性\n",
    "# 创建随机森林分类器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "# 使用随机森林分类器拟合数据\n",
    "rf.fit(X_train, y_train)\n",
    "# 获取特征重要性\n",
    "importance = rf.feature_importances_\n",
    "print(importance)\n",
    "# 创建选择器，并基于特征重要性进行特征选择\n",
    "selector = SelectFromModel(rf, threshold=1e-02)  # 设置重要性阈值\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# 获取选择后的特征矩阵\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "# 获取选择后的特征列名\n",
    "selected_features = X.columns[selector.get_support()]  # 使用columns获取列名\n",
    "print(\"选择后的特征矩阵:\")\n",
    "print(X_selected)\n",
    "print(\"选择后的特征列名:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########使用RLearner自身特征重要性\n",
    "##使用集成模型\n",
    "def make_models(nums,max_depth_list,n_estimators_list,learning_rate_list,min_child_weight,spw ):\n",
    "    models=[]\n",
    "    for i in range(nums):\n",
    "        models.append((str(i),xgb.XGBRegressor(max_depth=max_depth_list,learning_rate=0.06,gamma=0, min_child_weight = min_child_weight ,reg_alpha=0, # noqa: E501\n",
    "         n_estimators=n_estimators_list ,scale_pos_weight=spw)))\n",
    "    return models \n",
    "\n",
    "model1=make_models(3,200,200,0.06,spw=0.8,min_child_weight=0.5)  \n",
    "model2=make_models(3,260,125,0.06,spw=0,min_child_weight=4)\n",
    "stacking_model1 = StackingRegressor(estimators=model1,cv=5, n_jobs=5)\n",
    "stacking_model2 = StackingRegressor(estimators=model2,cv=5, n_jobs=5)\n",
    "\n",
    "\n",
    "rlearner = BaseRLearner(learner=None,\n",
    "        outcome_learner=stacking_model1,\n",
    "        effect_learner=stacking_model2,\n",
    "        ate_alpha=3,  \n",
    "        n_fold=5,\n",
    "        random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlearner.fit(X=X_train, treatment=t_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tau = rlearner.predict(X=X_train)\n",
    "model_tau_feature = RandomForestRegressor() \n",
    "# specify model for model_tau_feature\n",
    "importance =  rlearner.get_importance(X=X_train, tau=r_tau, model_tau_feature=model_tau_feature,\n",
    "                        normalize=True, method='auto',random_state=42)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the feature_importances_ method in the base learner (LGBMRegressor() in this example)\n",
    "rlearner.plot_importance(X=X_train, tau=r_tau, normalize=True, method='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########使用SHAP指标获得特征重要性\n",
    "shap_rlearner = rlearner.get_shap_values(X=X_train, tau=r_tau)\n",
    "\n",
    "# 使用这些列名绘制 SHAP 图\n",
    "rlearner.plot_shap_values(X=X_train, tau=r_tau,max_display=161)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_idx set to 'auto' (searches for feature with greatest approximate interaction)\n",
    "rlearner.plot_shap_dependence(treatment_group=1,\n",
    "                              feature_idx=1,\n",
    "                              X=X_train,\n",
    "                              tau=r_tau,\n",
    "                              interaction_idx='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########使用Filter方式获得特征重要性\n",
    "from causalml.metrics import *\n",
    "from causalml.feature_selection.filters import FilterSelect\n",
    "filter_method = FilterSelect()\n",
    "data = pd.read_csv('./data/-999_train.csv')  # 读取数据集\n",
    "data = data.iloc[:,1:]\n",
    "\n",
    "X = data.drop(['y','treatment'],axis=1) # 特征列\n",
    "y = data['y']  # 标签列\n",
    "treatment = data['treatment']  # 处理组标识\n",
    "feature_names = X.head(0)\n",
    "# F Filter with order 1\n",
    "method = 'ED'\n",
    "f_imp = filter_method.get_importance(data, feature_names, 'y', method, \n",
    "                                 experiment_group_column=\"treatment\",control_group=0,\n",
    "                                 treatment_group=1,\n",
    "                                 n_bins=5\n",
    "                                 ,order=2)\n",
    "\n",
    "\n",
    "print(f_imp.iloc[0:10,1])\n",
    "\n",
    "linear_importance = f_imp.iloc[:,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
